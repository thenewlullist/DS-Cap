init.cleaning <- function(){
      library(data.table);library(ngram);library(plyr);library(stylo);library(splitstackshape)
      con1 <- file("final/en_US/en_US.twitter.txt", "r")
      con2 <- file("final/en_US/en_US.news.txt", "r")
      con3 <- file("final/en_US/en_US.blogs.txt", "r")
      news<-readLines(con2,skipNul=T)
      news<-tolower(gsub("[^a-zA-Z \']", "", news))
      close(con2)
      blogs<-readLines(con3,skipNul=T)
      blogs<-tolower(gsub("[^a-zA-Z \']", "", blogs))
      close(con3)
      twit<-readLines(con1,skipNul=T)
      twit<-tolower(gsub("[^a-zA-Z \']", "", twit))
      close(con1)
      
      set.seed(1706)
      sample1<-sample.int(2360148,29333)
      sample2<-sample.int(1010242,9798)
      sample3<-sample.int(899288,8695)
      write(twit[sample1],file='final/en_US/Sample/tweetsample.txt')
      write(news[sample2],file='final/en_US/Sample/newssample.txt')
      write(blogs[sample3],file='final/en_US/Sample/blogssample.txt')
      
      con1 <- file('final/en_US/Sample/tweetsample.txt', "r")
      con2 <- file('final/en_US/Sample/newssample.txt', "r")
      con3 <- file('final/en_US/Sample/blogssample.txt', "r")
      news<-readLines(con2,skipNul=T)
      blogs<-readLines(con3,skipNul=T)
      twit<-readLines(con1,skipNul=T)
      close(con1);close(con2);close(con3)
      train<-concat(news,blogs,twit)
      train<-preprocess(train,case="lower")
      
      
      pent<-unlist(lapply(train[grep("[^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]*", train)], function(x) make.ngrams(txt.to.words(x, splitting.rule = "[ \t\n]+"), ngram.size = 5)))
      pent<-as.data.table(pent)
      pent<-cSplit(pent,splitCols="pent",sep=" ")
      #setnames(pent,colnames(pent),c('n1','n2','n3','n4','n5'))
      pent<-count(pent,colnames(pent))
      setkeyv(pent,colnames(pent))
      #pent<-subset(pent,freq>1)
      quad<-unlist(lapply(train[grep("[^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]*", train)], function(x) make.ngrams(txt.to.words(x, splitting.rule = "[ \t\n]+"), ngram.size = 4)))
      quad<-as.data.table(quad)
      quad<-cSplit(quad,splitCols="quad",sep=" ")
      setnames(quad,colnames(quad),c('n1','n2','n3','n4'))
      #quad[,c('n5'):=' ']
      quad<-count(quad,colnames(quad))
      setkeyv(quad,colnames(quad))
      #quad<-subset(quad,freq>1)
      tri <- unlist(lapply(train[grep("[^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]*", train)], function(x) make.ngrams(txt.to.words(x, splitting.rule = "[ \t\n]+"), ngram.size = 3)))
      tri<-as.data.table(tri)
      tri<-cSplit(tri,splitCols="tri",sep=" ")
      setnames(tri,colnames(tri),c('n1','n2','n3'))
      #tri[,c('n4','n5'):=' ']
      tri<-count(tri,colnames(tri))
      setkeyv(tri,colnames(tri))
      #tri<-subset(tri,freq>0)
      bi <- unlist(lapply(train[grep("[^ ]*[aeiouyAEIOUY]+[^ ]* [^ ]*[aeiouyAEIOUY]+[^ ]*", train)], function(x) make.ngrams(txt.to.words(x, splitting.rule = "[ \t\n]+"), ngram.size = 1)))
      bi<-as.data.table(bi)
      bi<-cSplit(bi,splitCols="bi",sep=" ")
      setnames(bi,colnames(bi),c('n1','n2'))
      #bi[,c('n3','n4','n5'):=' ']
      bi<-count(bi,colnames(bi))
      setkeyv(bi,colnames(bi))
      #bi<-subset(bi,freq>1)
      #traindt<-join_all(list(pent,quad,tri,bi),type='full')
      traindt<-as.data.table(tri)
      setkeyv(traindt,colnames(traindt))
      tri=NULL#;bi=NULL;quad=NULL;pent=NULL
      train=NULL;blogs=NULL;news=NULL;twit=NULL
      
      write.csv(traindt,"traindt.csv")
      #save.image("~/Documents/R Working Directory/final/en_US/Train/test.RData")
      
      return(head(traindt))
}